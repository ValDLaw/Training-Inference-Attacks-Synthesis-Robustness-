{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
>>>>>>> refs/remotes/origin/main
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
=======
    "\n",
>>>>>>> refs/remotes/origin/main
    "import torch\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "# from torchsummary import summary\n",
    "from torchinfo import summary\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# check OS is Window or Mac\n",
    "import platform\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "elif platform.system() == 'Darwin':\n",
    "    try:\n",
    "        device = torch.device(\"mps\")\n",
    "    except:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "float_formatter = \"{:.2f}\".format\n",
    "np.set_printoptions(formatter={'float_kind': float_formatter})\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
=======
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomFCNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=27648, out_features=100, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=100, out_features=32, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 2768462\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
>>>>>>> refs/remotes/origin/main
    "class CustomFCNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomFCNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
<<<<<<< HEAD
    "        self.fc1 = nn.Linear(19200, 128)  # Update fc1 input size to match the expected size\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 60)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(60, 10)\n",
=======
    "        self.fc1 = nn.Linear(3 * 96 * 96, 100)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(100, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
>>>>>>> refs/remotes/origin/main
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
<<<<<<< HEAD
    "class LeNet5V1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            # 1\n",
    "            nn.Conv2d(in_channels=3,    # cantidad de canales RGB == 3\n",
    "                      out_channels=6, \n",
    "                      kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # 2\n",
    "            nn.Conv2d(in_channels=6, \n",
    "                      out_channels=16,\n",
    "                      kernel_size=5, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=16*22*22, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=10),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.feature(x))"
=======
    "# Instantiate the model\n",
    "model = CustomFCNetwork()\n",
    "\n",
    "# Print the model architecture and number of trainable parameters\n",
    "print(model)\n",
    "print(f\"Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
>>>>>>> refs/remotes/origin/main
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    modelFCN = CustomFCNetwork()\n",
    "    model_name = \"FCN_final\"\n",
    "    modelFCN.load_state_dict(torch.load(f\"models/best_model_{model_name}.pth\", map_location=torch.device('cpu')))\n",
    "    modelFCN.eval()  # Set model to evaluation mode after loading\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
=======
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomFCNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=27648, out_features=100, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=100, out_features=32, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_uploaded = CustomFCNetwork()\n",
    "model_name = \"FCN\"\n",
    "model_uploaded.load_state_dict(torch.load(f\"./models//best_model_{model_name}.pth\"))\n",
    "\n",
    "model_uploaded.to(device)\n",
    "\n",
    "print(model_uploaded)"
>>>>>>> refs/remotes/origin/main
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    modelCNN = LeNet5V1()\n",
    "    model_name = \"LeNet5_final\"\n",
    "    modelCNN.load_state_dict(torch.load(f\"models/best_model_{model_name}.pth\", map_location=torch.device('cpu')))\n",
    "    modelCNN.eval()  # Set model to evaluation mode after loading\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training"
=======
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Definir la ruta de la carpeta\n",
    "path = \"data_split/test/\"\n",
    "\n",
    "# Crear listas para almacenar imágenes y etiquetas\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Iterar por las 10 clases\n",
    "for clse in range(1, 11):\n",
    "    # Obtener la ruta para cada clase\n",
    "    ruta = os.path.join(path, str(clse))\n",
    "    # Obtener los datos\n",
    "    for archivo in os.listdir(ruta):\n",
    "        if archivo.endswith(\".png\"):\n",
    "            ruta_img = os.path.join(ruta, archivo)\n",
    "            img = Image.open(ruta_img).convert(\"L\")\n",
    "            img_array = np.array(img)\n",
    "            images.append(img_array)\n",
    "            labels.append(clse)\n",
    "\n",
    "# Convertir las listas de imágenes y etiquetas a tensores de PyTorch\n",
    "images_tensor = torch.tensor(images, dtype=torch.float).view(-1, 1, 96, 96)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Crear un conjunto de datos PyTorch\n",
    "dataset = torch.utils.data.TensorDataset(images_tensor, labels_tensor)\n"
>>>>>>> refs/remotes/origin/main
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './img/'\n",
    "batch_size = 8 \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((80, 80)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(root='./data_split/train', transform=transform)\n",
    "val_dataset = ImageFolder(root='./data_split/test', transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generar adversial images\n",
    "def fgsm_attack(data, target, model, epsilon=0.03):\n",
    "    data.requires_grad = True\n",
    "    output = model(data)\n",
    "    loss = F.cross_entropy(output, target)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    perturbed_data = data + epsilon * data.grad.sign()\n",
    "    perturbed_data = torch.clamp(perturbed_data, 0, 1)  # Ensure pixel values are within valid range\n",
    "    return perturbed_data\n",
    "\n",
    "def adversarial_training(model, model_name):\n",
    "    # training loop\n",
    "    num_epochs = 10\n",
    "    epsilon = 0.03  #magnitud de la pertubación\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # genera adversarial images\n",
    "            adv_images = fgsm_attack(images, labels, model, epsilon)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(torch.cat((images, adv_images), dim=0))\n",
    "            combined_labels = torch.cat((labels, labels), dim=0)\n",
    "            loss = criterion(outputs, combined_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += combined_labels.size(0)\n",
    "            correct += predicted.eq(combined_labels).sum().item()\n",
    "        \n",
    "        # Print statistics\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] - Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        if (epoch == 8):  # Adjust the frequency of saving checkpoints as needed\n",
    "            checkpoint_path = \"./models/\" + f'adversarial_{model_name}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                # Add other relevant training parameters or state\n",
    "            }, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_training(modelFCN, \"FCN\")"
=======
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/enzoc/Escritorio/lab4ML/Training-Inference-Attacks-Synthesis-Robustness-/p3.ipynb Celda 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enzoc/Escritorio/lab4ML/Training-Inference-Attacks-Synthesis-Robustness-/p3.ipynb#W5sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata_split/test/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enzoc/Escritorio/lab4ML/Training-Inference-Attacks-Synthesis-Robustness-/p3.ipynb#W5sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# Cargar modelos\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/enzoc/Escritorio/lab4ML/Training-Inference-Attacks-Synthesis-Robustness-/p3.ipynb#W5sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m cnn_model \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39;49m\u001b[39m./models/best_model_LeNet5_final.pth\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enzoc/Escritorio/lab4ML/Training-Inference-Attacks-Synthesis-Robustness-/p3.ipynb#W5sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m fc_model \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39m\u001b[39m./models/best_model_FCN_final.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enzoc/Escritorio/lab4ML/Training-Inference-Attacks-Synthesis-Robustness-/p3.ipynb#W5sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39m# Lista de valores epsilon para variar la intensidad del ataque\u001b[39;00m\n",
      "\u001b[1;32m/home/enzoc/Escritorio/lab4ML/Training-Inference-Attacks-Synthesis-Robustness-/p3.ipynb Celda 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enzoc/Escritorio/lab4ML/Training-Inference-Attacks-Synthesis-Robustness-/p3.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(model_path):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/enzoc/Escritorio/lab4ML/Training-Inference-Attacks-Synthesis-Robustness-/p3.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(model_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enzoc/Escritorio/lab4ML/Training-Inference-Attacks-Synthesis-Robustness-/p3.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enzoc/Escritorio/lab4ML/Training-Inference-Attacks-Synthesis-Robustness-/p3.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[1;32m   1015\u001b[0m                      map_location,\n\u001b[1;32m   1016\u001b[0m                      pickle_module,\n\u001b[1;32m   1017\u001b[0m                      overall_storage\u001b[39m=\u001b[39;49moverall_storage,\n\u001b[1;32m   1018\u001b[0m                      \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m   1019\u001b[0m \u001b[39mif\u001b[39;00m mmap:\n\u001b[1;32m   1020\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmmap can only be used with files saved with \u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1424\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtorch.load.metadata\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mserialization_id\u001b[39m\u001b[39m\"\u001b[39m: zip_file\u001b[39m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1391\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1392\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1394\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1366\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         storage\u001b[39m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1363\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1366\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1367\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1368\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1371\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:381\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    380\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 381\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    382\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:274\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    273\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 274\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    275\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    276\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:258\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    255\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 258\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    259\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    260\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    261\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    262\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    263\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    264\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Definir la función FGSM\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Obtener el signo del gradiente\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Crear la imagen adversarial usando el signo del gradiente\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    # Asegurarse de que la imagen resultante esté en el rango [0, 1]\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "# Función para cargar el modelo\n",
    "def load_model(model_path):\n",
    "    model = torch.load(model_path)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Función para cargar y preprocesar una imagen\n",
    "def load_and_preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((96, 96)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    return Variable(image, requires_grad=True)\n",
    "\n",
    "# Función para realizar el ataque adversarial y mostrar las imágenes\n",
    "def perform_fgsm_attack(model, image, epsilon_values):\n",
    "    # Obtener la etiqueta verdadera de la imagen\n",
    "    true_label = torch.argmax(model(image).data).item()\n",
    "\n",
    "    # Inicializar la figura para mostrar las imágenes\n",
    "    plt.figure(figsize=(10, len(epsilon_values) * 2))\n",
    "\n",
    "    for i, epsilon in enumerate(epsilon_values):\n",
    "        # Calcular el gradiente de la pérdida con respecto a la entrada\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        model.zero_grad()\n",
    "        output = model(image)\n",
    "        loss_value = loss(output, torch.tensor([true_label]))\n",
    "        loss_value.backward()\n",
    "        data_grad = image.grad.data\n",
    "\n",
    "        # Realizar el ataque FGSM\n",
    "        perturbed_image = fgsm_attack(image, epsilon, data_grad)\n",
    "\n",
    "        # Mostrar la imagen perturbada\n",
    "        perturbed_label = torch.argmax(model(perturbed_image).data).item()\n",
    "        plt.subplot(len(epsilon_values), 2, 2 * i + 1)\n",
    "        plt.imshow(perturbed_image.squeeze().detach().numpy().transpose(1, 2, 0))\n",
    "        plt.title(f'FGSM Attack (Epsilon={epsilon})\\nPredicted Label: {perturbed_label}')\n",
    "\n",
    "        # Mostrar la diferencia entre la imagen original y la perturbada\n",
    "        difference = (perturbed_image - image).squeeze().detach().numpy().transpose(1, 2, 0)\n",
    "        plt.subplot(len(epsilon_values), 2, 2 * i + 2)\n",
    "        plt.imshow(difference)\n",
    "        plt.title('Difference')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ruta a las imágenes\n",
    "path = 'data_split/test/'\n",
    "\n",
    "# Cargar modelos\n",
    "cnn_model = load_model('/home/enzoc/Escritorio/lab4ML/Training-Inference-Attacks-Synthesis-Robustness-/models/best_model_LeNet5_final.pth')\n",
    "fc_model = load_model('/home/enzoc/Escritorio/lab4ML/Training-Inference-Attacks-Synthesis-Robustness-/models/best_model_FCN_final.pth')\n",
    "\n",
    "# Lista de valores epsilon para variar la intensidad del ataque\n",
    "epsilon_values = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "\n",
    "# Iterar sobre las clases y realizar el ataque adversarial\n",
    "for clse in range(1, 11):\n",
    "    # Obtener la ruta para cada clase\n",
    "    ruta = os.path.join(path, str(clse))\n",
    "    # Obtener los datos\n",
    "    for archivo in os.listdir(ruta):\n",
    "        if archivo.endswith(\".png\"):\n",
    "            ruta_img = os.path.join(ruta, archivo)\n",
    "\n",
    "            # Cargar y preprocesar la imagen\n",
    "            image = load_and_preprocess_image(ruta_img)\n",
    "\n",
    "            # Realizar el ataque adversarial en la CNN\n",
    "            print(f'Class: {clse}, CNN Model:')\n",
    "            perform_fgsm_attack(cnn_model, image, epsilon_values)\n",
    "\n",
    "            # Realizar el ataque adversarial en el modelo totalmente conectado\n",
    "            print(f'Class: {clse}, Fully Connected Model:')\n",
    "            perform_fgsm_attack(fc_model, image, epsilon_values)\n"
>>>>>>> refs/remotes/origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "adversarial_training(modelCNN,\"LeNet5\")"
   ]
=======
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> refs/remotes/origin/main
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": ".venv",
=======
   "display_name": "Python 3",
>>>>>>> refs/remotes/origin/main
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
